{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_sents_tags(test_corpus):\n",
    "    test_sents = []\n",
    "    test_tags = []\n",
    "    for sent in test_corpus:\n",
    "        test_sents.append([word for (word, tag) in sent])\n",
    "        test_tags.append([tag for (word, tag) in sent])\n",
    "    return test_sents, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(len(brown.tagged_sents()), n_iter=1, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in ss:\n",
    "    train_brown_word_tags = np.array(brown.tagged_sents())[train_index]\n",
    "    test_brown_word_tags = np.array(brown.tagged_sents())[test_index]\n",
    "    brown_test_sents, brown_test_tags = get_test_sents_tags(test_brown_word_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_tags(tagged_sents):\n",
    "    word_tags = []\n",
    "    for sent in tagged_sents:\n",
    "        word_tags.append((\"START\", \"START\"))\n",
    "        word_tags.extend([(tag[:2], word) for (word, tag) in sent])\n",
    "        word_tags.append((\"END\", \"END\"))\n",
    "    return word_tags\n",
    "indexes = [int(x) for x in np.linspace(0, len(train_brown_word_tags), 6)]\n",
    "train_brown1 = get_word_tags(train_brown_word_tags[:indexes[1]])\n",
    "train_brown2 = get_word_tags(train_brown_word_tags[:indexes[2]])\n",
    "train_brown3 = get_word_tags(train_brown_word_tags[:indexes[3]])\n",
    "train_brown4 = get_word_tags(train_brown_word_tags[:indexes[4]])\n",
    "train_brown5 = get_word_tags(train_brown_word_tags[:indexes[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of first chunk:  227837\n",
      "Size of second chunk:  458765\n",
      "Size of third chunk:  688937\n",
      "Size of fourth chunk:  917852\n",
      "Size of fifth chunk:  1150437\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of first chunk: \", len(train_brown1))\n",
    "print(\"Size of second chunk: \", len(train_brown2))\n",
    "print(\"Size of third chunk: \", len(train_brown3))\n",
    "print(\"Size of fourth chunk: \", len(train_brown4))\n",
    "print(\"Size of fifth chunk: \", len(train_brown5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_hmms(tags, cpd_tags, cpd_tagwords, cfd_tagwords, word_corpus, sentence):\n",
    "    distinct_tags = tags\n",
    "    #word_corpus = set(brown.words())\n",
    "    sentlen = len(sentence)\n",
    "\n",
    "    viterbi = []\n",
    "    backpointer = [ ]\n",
    "\n",
    "    # Inicializacion de las variables de viterbi_1 v_s(1)\n",
    "    first_viterbi = { }\n",
    "    first_backpointer = { }\n",
    "    for tag in distinct_tags:\n",
    "        # don't record anything for the START tag\n",
    "        if tag == \"START\": continue\n",
    "        first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[0] )\n",
    "        first_backpointer[ tag ] = \"START\"\n",
    "    viterbi.append(first_viterbi)\n",
    "    backpointer.append(first_backpointer)\n",
    "\n",
    "    currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
    "\n",
    "    for wordindex in range(1, len(sentence)):\n",
    "        this_viterbi = { }\n",
    "        this_backpointer = { }\n",
    "        prev_viterbi = viterbi[-1]\n",
    "\n",
    "        for tag in distinct_tags:\n",
    "            if tag == \"START\": continue\n",
    "            if sentence[wordindex] in word_corpus:\n",
    "                p_ws = cpd_tagwords[tag].prob(sentence[wordindex])\n",
    "            else:\n",
    "                p_ws = 1./cfd_tagwords[tag].N()\n",
    "            best_previous = max(prev_viterbi.keys(),\n",
    "                                key = lambda prevtag: \\\n",
    "                                prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * \n",
    "                                p_ws)\n",
    "\n",
    "            this_viterbi[ tag ] = prev_viterbi[ best_previous] * \\\n",
    "                cpd_tags[ best_previous ].prob(tag) * p_ws\n",
    "            this_backpointer[ tag ] = best_previous\n",
    "\n",
    "        currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
    "        #print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best two-tag sequence:\", \n",
    "        #      this_backpointer[ currbest], currbest)\n",
    "        viterbi.append(this_viterbi)\n",
    "        backpointer.append(this_backpointer)\n",
    "\n",
    "    prev_viterbi = viterbi[-1]\n",
    "    best_previous = max(prev_viterbi.keys(), key = lambda prevtag:\\\n",
    "                        prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
    "\n",
    "    prob_tagsequence = prev_viterbi[best_previous] * cpd_tags[ best_previous].prob(\"END\")\n",
    "\n",
    "    best_tagsequence = [\"END\", best_previous]\n",
    "    backpointer.reverse()\n",
    "\n",
    "    current_best_tag = best_previous\n",
    "    for bp in backpointer:\n",
    "        best_tagsequence.append(bp[current_best_tag])\n",
    "        current_best_tag = bp[current_best_tag]\n",
    "\n",
    "    best_tagsequence.reverse()\n",
    "    #print( \"The sentence was:\", end = \" \")\n",
    "    #for w in sentence: print( w, end = \" \")\n",
    "    #print(\"\\n\")\n",
    "    #print( \"The best tag sequence is:\", end = \" \")\n",
    "    #for t in best_tagsequence: print (t, end = \" \")\n",
    "    #print(\"\\n\")\n",
    "    #print( \"The probability of the best tag sequence is:\", prob_tagsequence)\n",
    "    return prob_tagsequence, best_tagsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateAccuracy(gs, calc, word_sec, word_dict):\n",
    "    _gs = gs[:]\n",
    "    _calc = calc[1:-1]\n",
    "    nc=kc=nt=kt=0.\n",
    "    for k in range(len(_gs)):\n",
    "        if _gs[k][:2]==_calc[k][:2]:\n",
    "            if word_sec[k] in word_dict:\n",
    "                kc+=1.\n",
    "            else:\n",
    "                nc+=1.\n",
    "        if word_sec[k] in word_dict:\n",
    "            kt+=1.\n",
    "        else:\n",
    "            nt+=1.\n",
    "    return (nc+kc)/(nt+kt),kc,kt,nc,nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_brown, test_sents, test_tags, index):\n",
    "    cfd_tagwords = nltk.ConditionalFreqDist(list(train_brown))\n",
    "    cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
    "\n",
    "    tags = [tag for (tag, word) in train_brown]\n",
    "    words = [word for (tag, word) in train_brown]\n",
    "    cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(tags))\n",
    "    cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "\n",
    "    distinct_tags = set(tags)\n",
    "    word_corpus = set(words)\n",
    "    #return brown_cfd_tagwords, brown_cpd_tagwords, brown_tags, brown_words, brown_cfd_tags\n",
    "\n",
    "    #distinct_tags, cpd_tags, cpd_tagwords, cfd_tagwords, word_corpus = datasets[dataset]\n",
    "    #test_sents, test_tags = [brown_test_sents, brown_test_tags]\n",
    "    accuracies = []\n",
    "    probabilities = []\n",
    "    j = 0\n",
    "    for i in range(len(test_sents)):\n",
    "        prob_tagsequence, best_tagsequence = train_hmms(distinct_tags, cpd_tags, cpd_tagwords, cfd_tagwords, word_corpus, test_sents[i])\n",
    "        accuracy,_,_,_,_ = evaluateAccuracy(test_tags[i], best_tagsequence, test_sents[i], word_corpus)\n",
    "        accuracies.append(accuracy)\n",
    "        probabilities.append(prob_tagsequence)\n",
    "        if j%500 == 0: print('Samples processed: ', j)\n",
    "        j += 1\n",
    "    #evaluation_dict[index] = accuracies\n",
    "    #prob_dict[index] = probabilities\n",
    "    print('Accuracy for {data} is {acc}'.format(data=index, acc=np.mean(accuracies)))\n",
    "    return accuracies, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples processed:  0\n",
      "Samples processed:  500\n",
      "Samples processed:  1000\n",
      "Samples processed:  1500\n",
      "Samples processed:  2000\n",
      "Samples processed:  2500\n",
      "Samples processed:  3000\n",
      "Samples processed:  3500\n",
      "Samples processed:  4000\n",
      "Samples processed:  4500\n",
      "Samples processed:  5000\n",
      "Samples processed:  5500\n",
      "Accuracy for 0 is 0.839853634683\n",
      "Samples processed:  0\n",
      "Samples processed:  500\n",
      "Samples processed:  1000\n",
      "Samples processed:  1500\n",
      "Samples processed:  2000\n",
      "Samples processed:  2500\n",
      "Samples processed:  3000\n",
      "Samples processed:  3500\n",
      "Samples processed:  4000\n",
      "Samples processed:  4500\n",
      "Samples processed:  5000\n",
      "Samples processed:  5500\n",
      "Accuracy for 1 is 0.883960873057\n",
      "Samples processed:  0\n"
     ]
    }
   ],
   "source": [
    "evaluation_dict = {}\n",
    "prob_dict = {}\n",
    "\n",
    "accuracies, probabilities = train_model(train_brown1, brown_test_sents, brown_test_tags, 0)\n",
    "evaluation_dict[0] = accuracies\n",
    "prob_dict[0] = probabilities\n",
    "accuracies, probabilities = train_model(train_brown2, brown_test_sents, brown_test_tags, 1)\n",
    "evaluation_dict[1] = accuracies\n",
    "prob_dict[1] = probabilities\n",
    "accuracies, probabilities = train_model(train_brown3, brown_test_sents, brown_test_tags, 2)\n",
    "evaluation_dict[2] = accuracies\n",
    "prob_dict[2] = probabilities\n",
    "accuracies, probabilities = train_model(train_brown4, brown_test_sents, brown_test_tags, 3)\n",
    "evaluation_dict[3] = accuracies\n",
    "prob_dict[3] = probabilities\n",
    "accuracies, probabilities = train_model(train_brown5, brown_test_sents, brown_test_tags, 4)\n",
    "evaluation_dict[4] = accuracies\n",
    "prob_dict[4] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
